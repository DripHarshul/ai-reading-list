### The "95% Signal" AI Reading List

This list aims to list together all the most important AI research papers into a single comprehensive list so that newcomers may learn quickly

| Name | Relevance / Importance | Link |
| :--- | :--- | :--- |
| **Welcome to the Era of Experience** | **The New Paradigm.** Silver & Sutton’s manifesto arguing AI is shifting from learning from human data (limited) to agentic experience (unlimited). | [PDF](https://www.google.com/search?q=http://incompleteideas.net/IncIdeas/SilverSutton2025.pdf) |
| **Attention Is All You Need** | **The Foundation.** The original Transformer paper. The bedrock of 99% of modern generative AI. | [arXiv](https://arxiv.org/abs/1706.03762) |
| **DeepSeekMath** *(GRPO Source)* | **RL for Reasoning.** Introduced **GRPO** (Group Relative Policy Optimization), the critical RL technique used in DeepSeek-R1/V3 to incentivize reasoning without a critic model. | [arXiv](https://arxiv.org/abs/2402.03300) |
| **Kimi K2: Open Agentic Intelligence** | **Agent Architecture.** Technical report for a 1T parameter MoE model designed specifically for long-horizon agentic tasks and "slow thinking." | [arXiv](https://arxiv.org/abs/2507.20534) |
| **Flow Matching for Generative Modeling** | **The "Flux" Paper.** The theoretical breakthrough (Flow Matching) that supersedes Diffusion, enabling the superior performance of FLUX.1 and SD3. | [arXiv](https://arxiv.org/abs/2210.02747) |
| **Scalable Diffusion Models with Transformers (DiT)** | **Vision Architecture.** Replaced U-Nets with Transformers, enabling the scaling of image/video generation (Sora, Flux, SD3). | [arXiv](https://arxiv.org/abs/2212.09748) |
| **Direct Preference Optimization (DPO)** | **Alignment.** Revolutionized RLHF by optimizing user preferences directly, removing the need for a complex separate reward model. | [arXiv](https://arxiv.org/abs/2305.18290) |
| **Mamba / Mamba-2** | **Efficient Context.** Introduced Selective State Spaces (SSMs), a linear-time alternative to Transformers for massive context windows. | [arXiv](https://arxiv.org/abs/2312.00752) |
| **GQA (Grouped Query Attention)** | **Inference Speed.** The standard attention mechanism for Llama 3 and DeepSeek, balancing speed (MQA) and quality (MHA). | [arXiv](https://arxiv.org/abs/2305.13245) |
| **Adam: A Method for Stochastic Optimization** | **The Engine.** The default optimizer used to train virtually every modern neural network foundation model. | [arXiv](https://arxiv.org/abs/1412.6980) |
| **The Bitter Lesson** | **Philosophy.** Rich Sutton’s essay arguing that general methods leveraging compute (search/learning) always beat human-engineered domain knowledge. | [Web](http://www.incompleteideas.net/IncIdeas/BitterLesson.html) |
| **FLUX.1 Kontext** | **In-Context Editing.** Introduces Flow Matching for in-context image editing and generation directly in latent space. | [Repo/Paper](https://github.com/black-forest-labs/flux) |
